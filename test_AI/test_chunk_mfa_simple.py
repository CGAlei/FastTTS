#!/usr/bin/env python3
"""
Simple focused test for MFA chunk processing
Tests whether MFA alignment is being applied per chunk vs. overall estimation
"""

import asyncio
import json
import logging
from tts.minimax_tts_engine import MinimaxTTSEngine

# Configure logging to see MFA processing details
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

async def test_mfa_chunking():
    """Test MFA chunking with a text that will definitely be split into chunks"""
    
    # Test text long enough to trigger chunking (>120 words when segmented)
    test_text = """Âú®Ëøô‰∏™Âø´ÈÄüÂèëÂ±ïÁöÑÊó∂‰ª£Ôºå‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÊ≠£Âú®Ê∑±ÂàªÂú∞ÊîπÂèòÁùÄÊàë‰ª¨ÁöÑÁîüÊ¥ªÊñπÂºèÂíåÂ∑•‰ΩúÊ®°Âºè„ÄÇ
    ‰ªéÁÆÄÂçïÁöÑËØ≠Èü≥ËØÜÂà´ÊäÄÊúØÂà∞Â§çÊùÇÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≥ªÁªüÔºå‰ªéÂü∫Á°ÄÁöÑÂõæÂÉèËØÜÂà´ÁÆóÊ≥ïÂà∞È´òÁ∫ßÁöÑËÆ°ÁÆóÊú∫ËßÜËßâÂ∫îÁî®Ôºå
    ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂ∫îÁî®ËåÉÂõ¥Ê≠£Âú®‰∏çÊñ≠Êâ©Â§ßÔºåÊ∏óÈÄèÂà∞Á§æ‰ºöÁöÑÂêÑ‰∏™È¢ÜÂüü„ÄÇÁâπÂà´ÊòØÂú®ÊïôËÇ≤Ë°å‰∏öÔºå
    Êô∫ËÉΩÂ≠¶‰π†Á≥ªÁªüËÉΩÂ§üÊ†πÊçÆÊØè‰∏™Â≠¶ÁîüÁöÑÂ≠¶‰π†ÁâπÁÇπÂíåËøõÂ∫¶Êèê‰æõ‰∏™ÊÄßÂåñÁöÑÂ≠¶‰π†ÊñπÊ°àÂíåÊïôÂ≠¶ÂÜÖÂÆπÔºå
    Â§ßÂ§ßÊèêÈ´ò‰∫ÜÂ≠¶‰π†ÊïàÁéáÂíåÊïôÂ≠¶Ë¥®Èáè„ÄÇÂêåÊó∂ÔºåÂú®ÂåªÁñóÂÅ•Â∫∑È¢ÜÂüüÔºå‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©ËØäÊñ≠Á≥ªÁªü
    ÂèØ‰ª•Â∏ÆÂä©ÂåªÁîüÊõ¥Âä†ÂáÜÁ°ÆÂø´ÈÄüÂú∞ËØÜÂà´ÂêÑÁßçÁñæÁóÖÁóáÁä∂Ôºå‰∏∫ÊÇ£ËÄÖÊèê‰æõÊõ¥Âä†Á≤æÂáÜÊúâÊïàÁöÑÊ≤ªÁñóÊñπÊ°à„ÄÇ
    Âú®ÈáëËûçÊúçÂä°Ë°å‰∏öÔºåÊô∫ËÉΩÈ£éÊéßÁ≥ªÁªüÈÄöËøáÂ§ßÊï∞ÊçÆÂàÜÊûêÂíåÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºå
    ËÉΩÂ§üÊúâÊïàËØÜÂà´ÂíåÈò≤ËåÉÂêÑÁßçÈáëËûçÈ£éÈô©Ôºå‰øùÊä§Áî®Êà∑ËµÑÈáëÂÆâÂÖ®„ÄÇ"""
    
    print("üß™ Testing MFA Chunk Processing")
    print("=" * 50)
    print(f"üìù Test text length: {len(test_text)} characters")
    
    try:
        # Initialize MiniMax TTS engine
        engine = MinimaxTTSEngine()
        
        if not engine.is_configured():
            print("‚ùå MiniMax engine not configured")
            return
        
        print("‚úÖ MiniMax engine configured")
        print(f"üìä Chunk size setting: {engine.chunk_size_words} words")
        
        # Test chunking logic first
        chunks = engine._split_text_into_chunks(test_text, engine.chunk_size_words)
        print(f"üìÑ Text will be split into {len(chunks)} chunks:")
        for i, chunk in enumerate(chunks, 1):
            print(f"   Chunk {i}: {len(chunk)} chars - '{chunk[:50]}...'")
        
        if len(chunks) == 1:
            print("‚ö†Ô∏è Text not long enough to trigger chunking - extending test...")
            test_text = test_text * 2  # Double the text
            chunks = engine._split_text_into_chunks(test_text, engine.chunk_size_words)
            print(f"üìÑ Extended text split into {len(chunks)} chunks")
        
        # Generate TTS with chunking (this will trigger MFA per chunk if available)
        print("\nüéØ Generating TTS with MFA chunk processing...")
        
        audio_bytes, word_timings = await engine.generate_speech(
            text=test_text,
            voice="moss_audio_96a80421-22ea-11f0-92db-0e8893cbb430",  # Aria voice
            speed=1.0,
            volume=0.8
        )
        
        print(f"‚úÖ TTS generation completed!")
        print(f"üìä Generated audio: {len(audio_bytes)} bytes")
        print(f"üìä Word timings: {len(word_timings)} words")
        
        # Analyze timing data to see MFA usage
        mfa_words = 0
        jieba_words = 0
        chunk_info_words = 0
        confidence_scores = []
        
        print("\nüîç Analyzing timing data sources:")
        for i, timing in enumerate(word_timings[:10]):  # Show first 10 words
            word = timing.get("word", "")
            source = timing.get("source", "unknown")
            confidence = timing.get("confidence", 0.0)
            is_chunk = timing.get("is_chunk_timing", False)
            
            print(f"   Word {i+1}: '{word}' | Source: {source} | Confidence: {confidence} | Chunk: {is_chunk}")
            
            if "mfa" in source.lower():
                mfa_words += 1
            elif "jieba" in source.lower():
                jieba_words += 1
            
            if is_chunk:
                chunk_info_words += 1
            
            if confidence > 0:
                confidence_scores.append(confidence)
        
        # Count total sources
        total_mfa = sum(1 for t in word_timings if "mfa" in t.get("source", "").lower())
        total_jieba = sum(1 for t in word_timings if "jieba" in t.get("source", "").lower() or t.get("source", "") == "")
        total_chunk_marked = sum(1 for t in word_timings if t.get("is_chunk_timing", False))
        
        print(f"\nüìà Timing Analysis Summary:")
        print(f"   Total words: {len(word_timings)}")
        print(f"   MFA-aligned words: {total_mfa} ({total_mfa/len(word_timings)*100:.1f}%)")
        print(f"   Jieba-estimated words: {total_jieba} ({total_jieba/len(word_timings)*100:.1f}%)")
        print(f"   Chunk-processed words: {total_chunk_marked} ({total_chunk_marked/len(word_timings)*100:.1f}%)")
        
        if confidence_scores:
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            print(f"   Average confidence: {avg_confidence:.3f}")
        
        # Determine if MFA chunking is working
        print(f"\nüéØ MFA Chunking Assessment:")
        if total_mfa > 0:
            print(f"   ‚úÖ MFA alignment is working! {total_mfa} words processed with MFA")
            if total_chunk_marked > 0:
                print(f"   ‚úÖ Chunk processing is active! {total_chunk_marked} words marked as chunk-processed")
            else:
                print(f"   ‚ö†Ô∏è MFA working but chunk metadata missing")
        else:
            print(f"   ‚ùå No MFA alignment detected - likely falling back to jieba estimation")
            print(f"   üí° This could mean:")
            print(f"      ‚Ä¢ MFA not installed or not available")
            print(f"      ‚Ä¢ MFA models missing")
            print(f"      ‚Ä¢ All chunk MFA attempts failed and fell back to jieba")
        
        # Save sample result for inspection
        sample_result = {
            "test_summary": {
                "text_length": len(test_text),
                "chunks_generated": len(chunks),
                "total_words": len(word_timings),
                "mfa_words": total_mfa,
                "jieba_words": total_jieba,
                "chunk_marked_words": total_chunk_marked,
                "avg_confidence": sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
            },
            "first_10_words": word_timings[:10],
            "chunk_sizes": [len(chunk) for chunk in chunks]
        }
        
        with open("mfa_chunk_test_sample.json", "w", encoding="utf-8") as f:
            json.dump(sample_result, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Sample results saved to: mfa_chunk_test_sample.json")
        
        # Conclusion
        if total_mfa > len(word_timings) * 0.5:
            print(f"\nüéâ SUCCESS: MFA chunk processing is working effectively!")
        elif total_mfa > 0:
            print(f"\n‚ö†Ô∏è PARTIAL: MFA chunk processing partially working - some chunks falling back to jieba")
        else:
            print(f"\n‚ùå ISSUE: MFA chunk processing not working - all words using jieba estimation")
            print(f"   üîß Check MFA installation and model availability")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_mfa_chunking())